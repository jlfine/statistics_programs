{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree from scratch\n",
    "\n",
    "Jacob L. Fine\n",
    "\n",
    "March 11th, 2025\n",
    "\n",
    "\n",
    "The decision tree is a way to classify data by recursively spliting data into smaller subgroups, constituting a tree. A 'split' will be defined as dividing a given set of samples into two smaller subgroups, where one of the two subgroups has all of its values for one of its features less than or equal to some threshold, and the other is greater than than that same threshold for that same feature. \n",
    "\n",
    "Let each node in the tree store a possible subset of the observations, and their ground truth labels. Each node will also store pointers to its two child nodes. Leaf nodes have no children, and only contain the value of the predicted class. Advantages of decisions include: (i) the complex nature of the classification process is made (often) interpretable as a series of simple decisions, (ii) they can handle well cases where the design matrix contains features that are both categorical and discrete, (iii) they capture well the non-linear nature of the relationships between features and class labels (iv) they are a non-parametric method that makes no assumptions about the distribution of the data, (v) they are fast to train and test on datasets with small/medium numbers of features and predictors.\n",
    "\n",
    "To classify a data point, represented as a $p$-dimensional vector, we traverse the graph by comparing each feature to the feature and its threshold represented at the node. This is just like how we follow a decision tree in most contexts. We then assign the class label to the data point based on the label of the leaf node.\n",
    "\n",
    "In the process of constructing the tree, we start with all the data at a root node, and perform the split on a given feature with a given threshold, such that the (feature, threshold) pair results in the least 'impurity' of the two child nodes (a left and right node) relative to all other possible (feature, threshold) splits. In a binary decision tree of continuous features, we use the convention that values less than the cutoff threshold are stored in the left child, whereas the remaining values are stored in teh right child. We do this recursively until the maximum depth is reached, or until all the values at a node are for the same class. \n",
    "\n",
    "The process of finding the best (feature, threshold) pair of values that provide the least impurity can be formalized as finding the threshold t of the jth feature that minimizes the Gini impurity, as follows:\n",
    "\n",
    "$$\\underset{(j,t)}{\\text{argmin}} \\space [w_{\\text{left}} I(N_{\\text{left}}) + w_{\\text{right}} I(N_{\\text{right}})]$$\n",
    "\n",
    "Where $I(N_{\\text{left}} = 1-\\sum_i p_i^2)$ is the Gini impurity of the left node, based on the distribution of class labels in that node (or equivalent for right node). And $w_{\\text{left}}$ is the proportion of class labels that went into the left node from the total samples. If we have $n$ samples, then $w_{\\text{left}} = n_{\\text{left}}/n$ is the proportion of samples that went into the left node. We may also use other measures to split the data, i.e., the information gain from the split.\n",
    "\n",
    "We will make a decision tree from scratch in Python, and then implement it on the iris dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# defining the node class for the decision tree\n",
    "class Node:\n",
    "    def __init__(self, split_feature=None,split_threshold=None,left_child=None,right_child=None,y_pred=None):\n",
    "        '''\n",
    "        Constructs the node.\n",
    "        split_feature = the feature X_j used to split the data at that node\n",
    "        split_threshold = the threshold (of one of the possible data values to split at for that feature)\n",
    "        left_child = the left child of the node \n",
    "        right_child = the right child of the node\n",
    "        y_pred = the predicted class of the node. Selected based on the most abundant class label in node. Only relevant if the leaf node. \n",
    "        '''\n",
    "        self.split_feature = split_feature\n",
    "        self.split_threshold = split_threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.y_pred = y_pred\n",
    "\n",
    "# definition the actual decision tree\n",
    "class DecisionTree:\n",
    "    # allows for users to select the max depth like in sk learn\n",
    "    def __init__(self,max_depth=int):  \n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def gini(self,y): # given distribution of labels at a node, get gini impurity; this is default here\n",
    "        class_labels, counts = np.unique(y, return_counts=True)  # gets the class ids and their counts\n",
    "        prob_y = counts/counts.sum() # normalizes the counts to get the distribution of class labels y at a node\n",
    "        gini = 1-np.sum(prob_y**2) # applies gini = 1 - sum(p^2)\n",
    "        return gini\n",
    "    \n",
    "    # Splits the data given a (feature, threshold)  pair\n",
    "    def split(self,X,y,split_feature, split_threshold):\n",
    "        left_row_indices = X[:,split_feature] < split_threshold # subsets the design matrix by rows less than the split threshold; row indices\n",
    "        right_row_indices = ~left_row_indices  # all row indices that are not in the left split\n",
    "        # returns four values: subset data matrix X corresponding and its class labels y for the split (two subsets)\n",
    "        return X[left_row_indices], y[left_row_indices], X[right_row_indices], y[right_row_indices] \n",
    "    \n",
    "    # finds the best (feature, threshold) pair to split data given the data at that node\n",
    "    # will iterate through each feature and threshold to find the best one\n",
    "    def find_best_split(self,X,y):\n",
    "\n",
    "        best_gini = float('inf') # starts gini impurity off as infinity; then gini values will be compared to it\n",
    "        best_feature, best_threshold = None, None  # will be updated until the best is found\n",
    "\n",
    "        for feature in range(X.shape[1]): # goes through each feature\n",
    "            thresholds = np.unique(X[:, feature])  # for that feature, gets all the possible data points (subset to unique cases)\n",
    "            for threshold in thresholds: # iterature through the thresholds\n",
    "                # calls the split function on the given feature and threshold; returns the subset data for that node\n",
    "                X_left, y_left, X_right, y_right = self.split(X,y,feature,threshold)  \n",
    "                if len(y_left) == 0 or len(y_right) == 0: # if there's no data in that split, keep going\n",
    "                    continue\n",
    "                \n",
    "                gini_left = self.gini(y_left)  # given the distribtion of labels in the prospective left child node, it computes the gini impurity\n",
    "                gini_right = self.gini(y_right)\n",
    "                weighted_sum_gini = (len(y_left)*gini_left + len(y_right)*gini_right)/len(y)  # gets the weigthed gini impurity for that split\n",
    "\n",
    "                if weighted_sum_gini < best_gini: # compares the computed gini to the current best gini\n",
    "                    # updates the best gini, feature and threshold accordingly.\n",
    "                    best_gini = weighted_sum_gini\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "                \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    # constructs the tree recursively by finding the best split at each node until max depth is reached; stores the current depth\n",
    "    # if all the labels are the same or no split is found, it just returns the node labelled by this class\n",
    "    def construct_tree(self, X, y, depth=0): \n",
    "        # first check the depth and number of unique y values stored at the node; will return labels to end the recursion process\n",
    "        if len(np.unique(y))==1 or depth >= self.max_depth: # once max deprth has been reached, or there's just one unique value in the list of y values\n",
    "            return Node(y_pred=np.bincount(y).argmax()) # store the class label for that node, as the most frequent count \n",
    "        \n",
    "        # given the data, finds the best (split_feature, split_threshold) pair to split the data by trying all combinations of features and thresholds.\n",
    "        split_feature, split_threshold = self.find_best_split(X,y)  \n",
    "\n",
    "\n",
    "        if split_feature is None: # if nothing was returned for the split feature, just return the most common class\n",
    "            return Node(y_pred=np.bincount(y).argmax())\n",
    "        \n",
    "        # uses the optimal (split_feature, split_threshold) we just found to split the data at the node\n",
    "        X_left, y_left, X_right, y_right = self.split(X,y,split_feature,split_threshold)  \n",
    "\n",
    "        # will now construct the children nodes for a node of interest based on the split done above, and recursively apply the function construct_tree to the offspring\n",
    "        left_child = self.construct_tree(X_left,y_left,depth+1)   # continues recursion on left side\n",
    "        right_child = self.construct_tree(X_right,y_right,depth+1)  # continues recrursion on right side\n",
    "\n",
    "        # at this line, since we haven't already returns the leaf nodes, it will return the non-leaf nodes that have references to their children\n",
    "        return Node(split_feature,split_threshold,left_child,right_child) \n",
    "    \n",
    "    # given the data, an nxp matrix X, and a p-dimensional vector y, fit the model to the decision tree. Store this to the Decision tree object by giving it the attribute 'root'\n",
    "    def fit(self,X,y):\n",
    "        self.root = self.construct_tree(X,y) # applies the function\n",
    "\n",
    "    # a function to classify any given one of the p-dimensional data points by traversing the tree (using recursion)\n",
    "    def classify_observation(self, x, node): \n",
    "        if node.y_pred is not None: # when we reach a leaf node, y_pred is filled, so we return its value\n",
    "            return node.y_pred\n",
    "        \n",
    "        # given the current node's split feature (which we stored before), \n",
    "        # we consider the value in x for that feature, and ask if it is less than the threshold\n",
    "        if x[node.split_feature] < node.split_threshold: \n",
    "            # we call the function again on the left child since by convention, left child nodes are those with values lower than the threshold\n",
    "            return self.classify_observation(x,node.left_child) \n",
    "        else:\n",
    "            # otherwise, we recur to the right node\n",
    "            return self.classify_observation(x,node.right_child) \n",
    "        \n",
    "    # given the design matrix, actually classify each sample by applying it through the classify_observation function   \n",
    "    def predict_classes(self,X):\n",
    "        # iterates through each of the n observations, and traverses the decision tree with it. \n",
    "        # Will return a n-dimensional vector of predicted classes.\n",
    "        return np.array([self.classify_observation(x,self.root) for x in X]) \n",
    "    \n",
    "    # will recursively print results of the tree\n",
    "    def display_tree(self, node=None, depth=0, feature_names=None, class_names=None, child_type=\"Root\"):\n",
    "\n",
    "        if node is None:\n",
    "            node = self.root  # starts with the root node\n",
    "\n",
    "        indent = \"  \" * depth  # indentation for readability\n",
    "\n",
    "        if node.y_pred is not None:\n",
    "            class_label = class_names[node.y_pred]  # gets the actual class name for the class y\n",
    "            print(f\"{indent}{child_type} -> leaf: class: {class_label}\")\n",
    "        else:\n",
    "            feature_name = feature_names[node.split_feature]  # gets the feature class name for the feature x_j\n",
    "            print(f\"{indent}{child_type} -> node: {feature_name} < {node.split_threshold}\")\n",
    "\n",
    "            # recursively applies function to print children\n",
    "            self.display_tree(node.left_child, depth + 1, feature_names, class_names, \"left child\")\n",
    "            self.display_tree(node.right_child, depth + 1, feature_names, class_names, \"right child\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# loads iris dataset\n",
    "iris = datasets.load_iris()\n",
    "features = iris.feature_names\n",
    "df = pd.DataFrame(iris.data, columns=features)\n",
    "df['species'] = [iris.target_names[value] for value in iris.target]\n",
    "\n",
    "# prepares data\n",
    "X = iris.data  # features\n",
    "y = iris.target  # labels\n",
    "\n",
    "df['y'] = list(y)  # also represents each species class as a number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Let's now implement our model with a train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42)\n",
    "\n",
    "# creating the DT object\n",
    "dt = DecisionTree(max_depth=3)\n",
    "\n",
    "# gits the model to our train and test data\n",
    "dt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root -> node: petal length (cm) < 3.0\n",
      "  left child -> leaf: class: setosa\n",
      "  right child -> node: petal width (cm) < 1.8\n",
      "    left child -> node: petal length (cm) < 5.6\n",
      "      left child -> leaf: class: versicolor\n",
      "      right child -> leaf: class: virginica\n",
      "    right child -> node: petal length (cm) < 4.9\n",
      "      left child -> leaf: class: virginica\n",
      "      right child -> leaf: class: virginica\n"
     ]
    }
   ],
   "source": [
    "# we can see the splits our model learned in the training process (the feature, threshold pairs at non-leaf nodes)\n",
    "dt.display_tree(feature_names=features, class_names=iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtained the predicted classes, now for the TEST data (based on the train data)\n",
    "y_pred_values = list(dt.predict_classes(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.9833\n"
     ]
    }
   ],
   "source": [
    "# now, we can get the accuracy, precision and recall\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Calculates the accuracy\n",
    "accuracy = round(accuracy_score(y_test, y_pred_values),4)\n",
    "\n",
    "print('accuracy =', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a great evaluation! We get a ~98% accuracy for the classification task, for the test data in our model. We did have a small dataset with features that seemed to provide discriminability between the classes. The next step after a decision tree is the Random Forest - which is an ensemble of decision trees. Here, we take random samples of observations (each observation is still a $p$-dimensional vector) and train a different decision tree on each one. We classify a data point based on the majority vote of the ensemble of decision trees. The number of trees in the ensemble is a hyperparameter, and is therefore chosen in advance of running the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
