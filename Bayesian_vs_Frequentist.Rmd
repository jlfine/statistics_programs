---
title: "Demonstrating Lindley's Paradox"
author: "Jacob L. Fine"
date: "April 27th, 2024"
output: html_document
---

An apparent paradox emerges when applying both the Bayesian and Frequentist approaches to hypothesis testing, where under certain conditions, we would reject $H_0$ when using a Frequentist approach but not under a Bayesian approach. Here, I will demonstrate cases where the p-value, $P(x|H_0)$ is small but the posterior probability that the null hypothesis is true given the data, $P(H_0|x)$, is large. The latter is used in Bayesian hypothesis testing to compute the posterior odds, $Odds = P(H_1|x)/P(H_0|x)$ which if sufficiently larger, entails us to reject $H_0$. 

Showing the relationship between the p-value, which is equivalent to $P(x|H_0)$ and the $P(H_0|x)$, we use Bayes' theorem:

$$ P(H_0|x) = \frac{P(x|H_0)P(H_0)}{P(x)} $$

We then use the law of total probability, using the weightings $P(H_1)$ and $P(H_0)$, to obtain that

$$P(x) = P(x|H_0)P(H_0) + P(x|H_1)P(H_1)$$

$$\implies P(H_0|x) = \frac{P(x|H_0)P(H_0)}{P(x|H_0)P(H_0) + P(x|H_1)P(H_1)} $$

Suppose we are modelling counts of the number of transcription start sites per gene and postulate that our data come from a Poisson distribution with an expected value of $\lambda = 4$ counts per gene under the null hypothesis. We therefore have, using the probability mass function (pmf) of the Poisson distribution:

$$ P(x|H_0) = \frac{e^{-4} 4^x}{x!} $$

$$\implies P(H_0|x) = \frac{\frac{e^{-4} 4^x}{x!}P(H_0)}{\frac{e^{-4} 4^x}{x!}P(H_0) + P(x|H_1)P(H_1)} $$

To deal with the term $ P(x|H_1)$, let us assume that $\lambda$ follows a normal distribution $N(0,1)$. The probability we are interested in can be expressed as

$$P(x|H_1) = f(x) = \int_{-\infty}^{\infty}f(x,\lambda)d\lambda$$
$$\implies f(x) = \int_{-\infty}^{\infty}f(x|\lambda)f(\lambda)d\lambda$$

But our pmf is not defined for negative values, so we can write

$$\implies f(x) = \int_{0}^{\infty} \frac{e^{-\lambda} \lambda^x}{x!} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{\lambda^2}{2}}d\lambda =  P(x|H_1)$$
This integral does not have a closed form but we can solve it numerically in R. Substituting this into our result from Bayes' theorem, we have

$$\implies P(H_0|x) = \frac{\frac{e^{-4} 4^x}{x!}P(H_0)}{\frac{e^{-4} 4^x}{x!}P(H_0) + ( \int_{0}^{\infty} \frac{e^{-\lambda} \lambda^x}{x!} \cdot \frac{1}{\sqrt{2\pi}} e^{-\frac{\lambda^2}{2}}d\lambda)P(H_1)} $$

We may now wish to evaluate $P(H_0|x)$ and $P(H_1|x)$ at different values of $x$, and for different choices of priors. We want to show that $P(H_0|x)$ can be large while $P(x|H_0)$ is small, leading to an apparent contradiction in when the null hypothesis is rejected.

```{r}
# import ggplot
library(ggplot2)

# a function to compute the probabilities of interest for a specific value of P_H0
compute_probabilities_single <- function(x_values, P_H0) {
  # since H0 and H1 are two complementary events so we can use pA = 1 - pB
  P_H1 <- 1 - P_H0
  
  # we now initialize an empty dataframe to store results
  results_df <- data.frame(
    x = numeric(length(x_values)), 
    P_x_H0 = numeric(length(x_values)), 
    P_H0_given_x = numeric(length(x_values)), 
    P_H0 = P_H0,  # Add P_H0 and P_H1 directly to the dataframe
    P_H1 = P_H1,
    posterior_odds = numeric(length(x_values))
  )
  
  # for each value of x, we calculate the probabilities
  for (i in seq_along(x_values)) {
    # selects value of x
    x <- x_values[i]
    # adds current values of x, P_x_H0, and P_H0_x to dataframe
    results_df[i, "x"] <- x
    # computes the p-value
    results_df[i, "P_x_H0"] <- exp(-4) * (4^x) / factorial(x)
    # computes P(H0_given_x)
    numerator <- results_df[i, "P_x_H0"] * P_H0
    # finds the integral numerically
    denominator <- numerator + integrate(function(lambda) (exp(-lambda) * (lambda^x) / factorial(x) * (1 / sqrt(2 * pi)) * exp(-(lambda^2) / 2)), 0, Inf)$value * P_H1
    results_df[i, "P_H0_given_x"] <- numerator / denominator
    
    # computes the posterior odds
    results_df[i, "posterior_odds"] <- (1 - results_df[i, "P_H0_given_x"]) / results_df[i, "P_H0_given_x"]
  }
  
  return(results_df)
}

# generates a list of input values x
x_values <- seq(0, 100)

# specify the values of P(H0) you want to use
P_H0_values <- c(0.5, 0.7, 0.9)

# initialize a list to store plots
plot_list <- list()

# loop through each value of P(H0) and generate plots
for (P_H0 in P_H0_values) {
  # compute probabilities for the specified P_H0
  plot_df <- compute_probabilities_single(x_values, P_H0)
  
  # Plot P(x | H0) and P(H0 | x) versus x
  p <- ggplot(plot_df, aes(x = x)) +
    geom_line(aes(y = P_x_H0, color = "P(x | H0)")) +
    geom_line(aes(y = P_H0_given_x, color = "P(H0 | x)")) +
    geom_vline(xintercept = 4, linetype = "dashed", color = "purple") +  # Add vertical line at x = 4
    geom_hline(yintercept = 0.05, linetype = "dashed", color = "darkgreen") +  # Add horizontal line at y = 0.05
    labs(x = "x", y = "Probability", color = "Legend") +
    ggtitle(paste0("p-value and P(H0|x) as a function of x, for P(H0) = ", P_H0, "")) +
    scale_color_manual(
      values = c("P(x | H0)" = "blue", "P(H0 | x)" = "red"),
      labels = c("P(x | H0)" = "p-value", "P(H0 | x)" = "P(H0|x)", "purple" = "x = 4", "darkgreen" = "y = 0.05")
    )
  
  # Add the plot to the plot list
  plot_list[[as.character(P_H0)]] <- p
}

# Print the plots
print(plot_list$`0.5`)
print(plot_list$`0.7`)
print(plot_list$`0.9`)

```
